{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 305.9148254394531\n",
      "Iteration 100, Loss: 365.1031188964844\n",
      "Iteration 200, Loss: 416.9606018066406\n",
      "Iteration 300, Loss: 461.61090087890625\n",
      "Iteration 400, Loss: 505.6009826660156\n",
      "Iteration 500, Loss: 549.9507446289062\n",
      "Iteration 600, Loss: 594.453369140625\n",
      "Iteration 700, Loss: 638.9197387695312\n",
      "Iteration 800, Loss: 683.17724609375\n",
      "Iteration 900, Loss: 727.0724487304688\n",
      "Iteration 1000, Loss: 770.4666137695312\n",
      "Iteration 1100, Loss: 813.239013671875\n",
      "Iteration 1200, Loss: 855.283447265625\n",
      "Iteration 1300, Loss: 896.5091552734375\n",
      "Iteration 1400, Loss: 936.8383178710938\n",
      "Iteration 1500, Loss: 975.2160034179688\n",
      "Iteration 1600, Loss: 1009.8627319335938\n",
      "Iteration 1700, Loss: 1041.8621826171875\n",
      "Iteration 1800, Loss: 1071.4053955078125\n",
      "Iteration 1900, Loss: 1098.61962890625\n",
      "Iteration 2000, Loss: 1123.6383056640625\n",
      "Iteration 2100, Loss: 1146.598876953125\n",
      "Iteration 2200, Loss: 1167.636474609375\n",
      "Iteration 2300, Loss: 1186.884765625\n",
      "Iteration 2400, Loss: 1204.4739990234375\n",
      "Iteration 2500, Loss: 1220.5291748046875\n",
      "Iteration 2600, Loss: 1235.168701171875\n",
      "Iteration 2700, Loss: 1248.5050048828125\n",
      "Iteration 2800, Loss: 1260.6448974609375\n",
      "Iteration 2900, Loss: 1271.687255859375\n",
      "Iteration 3000, Loss: 1281.72412109375\n",
      "Iteration 3100, Loss: 1290.841552734375\n",
      "Iteration 3200, Loss: 1299.20947265625\n",
      "Iteration 3300, Loss: 1307.938720703125\n",
      "Iteration 3400, Loss: 1317.3880615234375\n",
      "Iteration 3500, Loss: 1327.62109375\n",
      "Iteration 3600, Loss: 1338.707275390625\n",
      "Iteration 3700, Loss: 1350.72265625\n",
      "Iteration 3800, Loss: 1363.751708984375\n",
      "Iteration 3900, Loss: 1377.88671875\n",
      "Iteration 4000, Loss: 1393.233154296875\n",
      "Iteration 4100, Loss: 1409.8990478515625\n",
      "Iteration 4200, Loss: 1428.010986328125\n",
      "Iteration 4300, Loss: 1447.223388671875\n",
      "Iteration 4400, Loss: 1464.4443359375\n",
      "Iteration 4500, Loss: 1479.3658447265625\n",
      "Iteration 4600, Loss: 1492.2802734375\n",
      "Iteration 4700, Loss: 1503.447509765625\n",
      "Iteration 4800, Loss: 1513.0943603515625\n",
      "Iteration 4900, Loss: 1521.4222412109375\n",
      "Iteration 5000, Loss: 1528.607666015625\n",
      "Iteration 5100, Loss: 1534.8028564453125\n",
      "Iteration 5200, Loss: 1540.1429443359375\n",
      "Iteration 5300, Loss: 1544.743896484375\n",
      "Iteration 5400, Loss: 1548.7064208984375\n",
      "Iteration 5500, Loss: 1552.1181640625\n",
      "Iteration 5600, Loss: 1555.0555419921875\n",
      "Iteration 5700, Loss: 1557.5831298828125\n",
      "Iteration 5800, Loss: 1559.7581787109375\n",
      "Iteration 5900, Loss: 1561.629638671875\n",
      "Iteration 6000, Loss: 1563.2391357421875\n",
      "Iteration 6100, Loss: 1564.6236572265625\n",
      "Iteration 6200, Loss: 1565.814453125\n",
      "Iteration 6300, Loss: 1566.83837890625\n",
      "Iteration 6400, Loss: 1567.7193603515625\n",
      "Iteration 6500, Loss: 1568.4765625\n",
      "Iteration 6600, Loss: 1569.1343994140625\n",
      "Iteration 6700, Loss: 1569.7161865234375\n",
      "Iteration 6800, Loss: 1570.2313232421875\n",
      "Iteration 6900, Loss: 1570.68701171875\n",
      "Iteration 7000, Loss: 1571.09033203125\n",
      "Iteration 7100, Loss: 1571.44775390625\n",
      "Iteration 7200, Loss: 1571.7645263671875\n",
      "Iteration 7300, Loss: 1572.0443115234375\n",
      "Iteration 7400, Loss: 1572.2919921875\n",
      "Iteration 7500, Loss: 1572.5113525390625\n",
      "Iteration 7600, Loss: 1572.705322265625\n",
      "Iteration 7700, Loss: 1572.8768310546875\n",
      "Iteration 7800, Loss: 1573.029052734375\n",
      "Iteration 7900, Loss: 1573.1636962890625\n",
      "Iteration 8000, Loss: 1573.2823486328125\n",
      "Iteration 8100, Loss: 1573.3876953125\n",
      "Iteration 8200, Loss: 1573.481201171875\n",
      "Iteration 8300, Loss: 1573.5640869140625\n",
      "Iteration 8400, Loss: 1573.637939453125\n",
      "Iteration 8500, Loss: 1573.700927734375\n",
      "Iteration 8600, Loss: 1573.75927734375\n",
      "Iteration 8700, Loss: 1573.808349609375\n",
      "Iteration 8800, Loss: 1573.855224609375\n",
      "Iteration 8900, Loss: 1573.8922119140625\n",
      "Iteration 9000, Loss: 1573.9290771484375\n",
      "Iteration 9100, Loss: 1573.959716796875\n",
      "Iteration 9200, Loss: 1573.9844970703125\n",
      "Iteration 9300, Loss: 1574.009033203125\n",
      "Iteration 9400, Loss: 1574.0338134765625\n",
      "Iteration 9500, Loss: 1574.0538330078125\n",
      "Iteration 9600, Loss: 1574.066162109375\n",
      "Iteration 9700, Loss: 1574.07861328125\n",
      "Iteration 9800, Loss: 1574.0906982421875\n",
      "Iteration 9900, Loss: 1574.1031494140625\n",
      "Iteration 10000, Loss: 1574.115234375\n",
      "Iteration 10100, Loss: 1574.127685546875\n",
      "Iteration 10200, Loss: 1574.1397705078125\n",
      "Iteration 10300, Loss: 1574.1497802734375\n",
      "Iteration 10400, Loss: 1574.1497802734375\n",
      "Iteration 10500, Loss: 1574.1497802734375\n",
      "Iteration 10600, Loss: 1574.1497802734375\n",
      "Iteration 10700, Loss: 1574.1497802734375\n",
      "Iteration 10800, Loss: 1574.1497802734375\n",
      "Iteration 10900, Loss: 1574.1497802734375\n",
      "Iteration 11000, Loss: 1574.1497802734375\n",
      "Iteration 11100, Loss: 1574.1497802734375\n",
      "Iteration 11200, Loss: 1574.1497802734375\n",
      "Iteration 11300, Loss: 1574.1497802734375\n",
      "Iteration 11400, Loss: 1574.1497802734375\n",
      "Iteration 11500, Loss: 1574.1497802734375\n",
      "Iteration 11600, Loss: 1574.1497802734375\n",
      "Iteration 11700, Loss: 1574.1497802734375\n",
      "Iteration 11800, Loss: 1574.1497802734375\n",
      "Iteration 11900, Loss: 1574.1497802734375\n",
      "Iteration 12000, Loss: 1574.1497802734375\n",
      "Iteration 12100, Loss: 1574.1497802734375\n",
      "Iteration 12200, Loss: 1574.1497802734375\n",
      "Iteration 12300, Loss: 1574.1497802734375\n",
      "Iteration 12400, Loss: 1574.1497802734375\n",
      "Iteration 12500, Loss: 1574.1497802734375\n",
      "Iteration 12600, Loss: 1574.1497802734375\n",
      "Iteration 12700, Loss: 1574.1497802734375\n",
      "Iteration 12800, Loss: 1574.1497802734375\n",
      "Iteration 12900, Loss: 1574.1497802734375\n",
      "Iteration 13000, Loss: 1574.1497802734375\n",
      "Iteration 13100, Loss: 1574.1497802734375\n",
      "Iteration 13200, Loss: 1574.1497802734375\n",
      "Iteration 13300, Loss: 1574.1497802734375\n",
      "Iteration 13400, Loss: 1574.1497802734375\n",
      "Iteration 13500, Loss: 1574.1497802734375\n",
      "Iteration 13600, Loss: 1574.1497802734375\n",
      "Iteration 13700, Loss: 1574.1497802734375\n",
      "Iteration 13800, Loss: 1574.1497802734375\n",
      "Iteration 13900, Loss: 1574.1497802734375\n",
      "Iteration 14000, Loss: 1574.1497802734375\n",
      "Iteration 14100, Loss: 1574.1497802734375\n",
      "Iteration 14200, Loss: 1574.1497802734375\n",
      "Iteration 14300, Loss: 1574.1497802734375\n",
      "Iteration 14400, Loss: 1574.1497802734375\n",
      "Iteration 14500, Loss: 1574.1497802734375\n",
      "Iteration 14600, Loss: 1574.1497802734375\n",
      "Iteration 14700, Loss: 1574.1497802734375\n",
      "Iteration 14800, Loss: 1574.1497802734375\n",
      "Iteration 14900, Loss: 1574.1497802734375\n",
      "Iteration 15000, Loss: 1574.1497802734375\n",
      "Iteration 15100, Loss: 1574.1497802734375\n",
      "Iteration 15200, Loss: 1574.1497802734375\n",
      "Iteration 15300, Loss: 1574.1497802734375\n",
      "Iteration 15400, Loss: 1574.1497802734375\n",
      "Iteration 15500, Loss: 1574.1497802734375\n",
      "Iteration 15600, Loss: 1574.1497802734375\n",
      "Iteration 15700, Loss: 1574.1497802734375\n",
      "Iteration 15800, Loss: 1574.1497802734375\n",
      "Iteration 15900, Loss: 1574.1497802734375\n",
      "Iteration 16000, Loss: 1574.1497802734375\n",
      "Iteration 16100, Loss: 1574.1497802734375\n",
      "Iteration 16200, Loss: 1574.1497802734375\n",
      "Iteration 16300, Loss: 1574.1497802734375\n",
      "Iteration 16400, Loss: 1574.1497802734375\n",
      "Iteration 16500, Loss: 1574.1497802734375\n",
      "Iteration 16600, Loss: 1574.1497802734375\n",
      "Iteration 16700, Loss: 1574.1497802734375\n",
      "Iteration 16800, Loss: 1574.1497802734375\n",
      "Iteration 16900, Loss: 1574.1497802734375\n",
      "Iteration 17000, Loss: 1574.1497802734375\n",
      "Iteration 17100, Loss: 1574.1497802734375\n",
      "Iteration 17200, Loss: 1574.1497802734375\n",
      "Iteration 17300, Loss: 1574.1497802734375\n",
      "Iteration 17400, Loss: 1574.1497802734375\n",
      "Iteration 17500, Loss: 1574.1497802734375\n",
      "Iteration 17600, Loss: 1574.1497802734375\n",
      "Iteration 17700, Loss: 1574.1497802734375\n",
      "Iteration 17800, Loss: 1574.1497802734375\n",
      "Iteration 17900, Loss: 1574.1497802734375\n",
      "Iteration 18000, Loss: 1574.1497802734375\n",
      "Iteration 18100, Loss: 1574.1497802734375\n",
      "Iteration 18200, Loss: 1574.1497802734375\n",
      "Iteration 18300, Loss: 1574.1497802734375\n",
      "Iteration 18400, Loss: 1574.1497802734375\n",
      "Iteration 18500, Loss: 1574.1497802734375\n",
      "Iteration 18600, Loss: 1574.1497802734375\n",
      "Iteration 18700, Loss: 1574.1497802734375\n",
      "Iteration 18800, Loss: 1574.1497802734375\n",
      "Iteration 18900, Loss: 1574.1497802734375\n",
      "Iteration 19000, Loss: 1574.1497802734375\n",
      "Iteration 19100, Loss: 1574.1497802734375\n",
      "Iteration 19200, Loss: 1574.1497802734375\n",
      "Iteration 19300, Loss: 1574.1497802734375\n",
      "Iteration 19400, Loss: 1574.1497802734375\n",
      "Iteration 19500, Loss: 1574.1497802734375\n",
      "Iteration 19600, Loss: 1574.1497802734375\n",
      "Iteration 19700, Loss: 1574.1497802734375\n",
      "Iteration 19800, Loss: 1574.1497802734375\n",
      "Iteration 19900, Loss: 1574.1497802734375\n",
      "Iteration 20000, Loss: 1574.1497802734375\n",
      "Iteration 20100, Loss: 1574.1497802734375\n",
      "Iteration 20200, Loss: 1574.1497802734375\n",
      "Iteration 20300, Loss: 1574.1497802734375\n",
      "Iteration 20400, Loss: 1574.1497802734375\n",
      "Iteration 20500, Loss: 1574.1497802734375\n",
      "Iteration 20600, Loss: 1574.1497802734375\n",
      "Iteration 20700, Loss: 1574.1497802734375\n",
      "Iteration 20800, Loss: 1574.1497802734375\n",
      "Iteration 20900, Loss: 1574.1497802734375\n",
      "Iteration 21000, Loss: 1574.1497802734375\n",
      "Iteration 21100, Loss: 1574.1497802734375\n",
      "Iteration 21200, Loss: 1574.1497802734375\n",
      "Iteration 21300, Loss: 1574.1497802734375\n",
      "Iteration 21400, Loss: 1574.1497802734375\n",
      "Iteration 21500, Loss: 1574.1497802734375\n",
      "Iteration 21600, Loss: 1574.1497802734375\n",
      "Iteration 21700, Loss: 1574.1497802734375\n",
      "Iteration 21800, Loss: 1574.1497802734375\n",
      "Iteration 21900, Loss: 1574.1497802734375\n",
      "Iteration 22000, Loss: 1574.1497802734375\n",
      "Iteration 22100, Loss: 1574.1497802734375\n",
      "Iteration 22200, Loss: 1574.1497802734375\n",
      "Iteration 22300, Loss: 1574.1497802734375\n",
      "Iteration 22400, Loss: 1574.1497802734375\n",
      "Iteration 22500, Loss: 1574.1497802734375\n",
      "Iteration 22600, Loss: 1574.1497802734375\n",
      "Iteration 22700, Loss: 1574.1497802734375\n",
      "Iteration 22800, Loss: 1574.1497802734375\n",
      "Iteration 22900, Loss: 1574.1497802734375\n",
      "Iteration 23000, Loss: 1574.1497802734375\n",
      "Iteration 23100, Loss: 1574.1497802734375\n",
      "Iteration 23200, Loss: 1574.1497802734375\n",
      "Iteration 23300, Loss: 1574.1497802734375\n",
      "Iteration 23400, Loss: 1574.1497802734375\n",
      "Iteration 23500, Loss: 1574.1497802734375\n",
      "Iteration 23600, Loss: 1574.1497802734375\n",
      "Iteration 23700, Loss: 1574.1497802734375\n",
      "Iteration 23800, Loss: 1574.1497802734375\n",
      "Iteration 23900, Loss: 1574.1497802734375\n",
      "Iteration 24000, Loss: 1574.1497802734375\n",
      "Iteration 24100, Loss: 1574.1497802734375\n",
      "Iteration 24200, Loss: 1574.1497802734375\n",
      "Iteration 24300, Loss: 1574.1497802734375\n",
      "Iteration 24400, Loss: 1574.1497802734375\n",
      "Iteration 24500, Loss: 1574.1497802734375\n",
      "Iteration 24600, Loss: 1574.1497802734375\n",
      "Iteration 24700, Loss: 1574.1497802734375\n",
      "Iteration 24800, Loss: 1574.1497802734375\n",
      "Iteration 24900, Loss: 1574.1497802734375\n",
      "Iteration 25000, Loss: 1574.1497802734375\n",
      "Iteration 25100, Loss: 1574.1497802734375\n",
      "Iteration 25200, Loss: 1574.1497802734375\n",
      "Iteration 25300, Loss: 1574.1497802734375\n",
      "Iteration 25400, Loss: 1574.1497802734375\n",
      "Iteration 25500, Loss: 1574.1497802734375\n",
      "Iteration 25600, Loss: 1574.1497802734375\n",
      "Iteration 25700, Loss: 1574.1497802734375\n",
      "Iteration 25800, Loss: 1574.1497802734375\n",
      "Iteration 25900, Loss: 1574.1497802734375\n",
      "Iteration 26000, Loss: 1574.1497802734375\n",
      "Iteration 26100, Loss: 1574.1497802734375\n",
      "Iteration 26200, Loss: 1574.1497802734375\n",
      "Iteration 26300, Loss: 1574.1497802734375\n",
      "Iteration 26400, Loss: 1574.1497802734375\n",
      "Iteration 26500, Loss: 1574.1497802734375\n",
      "Iteration 26600, Loss: 1574.1497802734375\n",
      "Iteration 26700, Loss: 1574.1497802734375\n",
      "Iteration 26800, Loss: 1574.1497802734375\n",
      "Iteration 26900, Loss: 1574.1497802734375\n",
      "Iteration 27000, Loss: 1574.1497802734375\n",
      "Iteration 27100, Loss: 1574.1497802734375\n",
      "Iteration 27200, Loss: 1574.1497802734375\n",
      "Iteration 27300, Loss: 1574.1497802734375\n",
      "Iteration 27400, Loss: 1574.1497802734375\n",
      "Iteration 27500, Loss: 1574.1497802734375\n",
      "Iteration 27600, Loss: 1574.1497802734375\n",
      "Iteration 27700, Loss: 1574.1497802734375\n",
      "Iteration 27800, Loss: 1574.1497802734375\n",
      "Iteration 27900, Loss: 1574.1497802734375\n",
      "Iteration 28000, Loss: 1574.1497802734375\n",
      "Iteration 28100, Loss: 1574.1497802734375\n",
      "Iteration 28200, Loss: 1574.1497802734375\n",
      "Iteration 28300, Loss: 1574.1497802734375\n",
      "Iteration 28400, Loss: 1574.1497802734375\n",
      "Iteration 28500, Loss: 1574.1497802734375\n",
      "Iteration 28600, Loss: 1574.1497802734375\n",
      "Iteration 28700, Loss: 1574.1497802734375\n",
      "Iteration 28800, Loss: 1574.1497802734375\n",
      "Iteration 28900, Loss: 1574.1497802734375\n",
      "Iteration 29000, Loss: 1574.1497802734375\n",
      "Iteration 29100, Loss: 1574.1497802734375\n",
      "Iteration 29200, Loss: 1574.1497802734375\n",
      "Iteration 29300, Loss: 1574.1497802734375\n",
      "Iteration 29400, Loss: 1574.1497802734375\n",
      "Iteration 29500, Loss: 1574.1497802734375\n",
      "Iteration 29600, Loss: 1574.1497802734375\n",
      "Iteration 29700, Loss: 1574.1497802734375\n",
      "Iteration 29800, Loss: 1574.1497802734375\n",
      "Iteration 29900, Loss: 1574.1497802734375\n",
      "Iteration 30000, Loss: 1574.1497802734375\n",
      "Iteration 30100, Loss: 1574.1497802734375\n",
      "Iteration 30200, Loss: 1574.1497802734375\n",
      "Iteration 30300, Loss: 1574.1497802734375\n",
      "Iteration 30400, Loss: 1574.1497802734375\n",
      "Iteration 30500, Loss: 1574.1497802734375\n",
      "Iteration 30600, Loss: 1574.1497802734375\n",
      "Iteration 30700, Loss: 1574.1497802734375\n",
      "Iteration 30800, Loss: 1574.1497802734375\n",
      "Iteration 30900, Loss: 1574.1497802734375\n",
      "Iteration 31000, Loss: 1574.1497802734375\n",
      "Iteration 31100, Loss: 1574.1497802734375\n",
      "Iteration 31200, Loss: 1574.1497802734375\n",
      "Iteration 31300, Loss: 1574.1497802734375\n",
      "Iteration 31400, Loss: 1574.1497802734375\n",
      "Iteration 31500, Loss: 1574.1497802734375\n",
      "Iteration 31600, Loss: 1574.1497802734375\n",
      "Iteration 31700, Loss: 1574.1497802734375\n",
      "Iteration 31800, Loss: 1574.1497802734375\n",
      "Iteration 31900, Loss: 1574.1497802734375\n",
      "Iteration 32000, Loss: 1574.1497802734375\n",
      "Iteration 32100, Loss: 1574.1497802734375\n",
      "Iteration 32200, Loss: 1574.1497802734375\n",
      "Iteration 32300, Loss: 1574.1497802734375\n",
      "Iteration 32400, Loss: 1574.1497802734375\n",
      "Iteration 32500, Loss: 1574.1497802734375\n",
      "Iteration 32600, Loss: 1574.1497802734375\n",
      "Iteration 32700, Loss: 1574.1497802734375\n",
      "Iteration 32800, Loss: 1574.1497802734375\n",
      "Iteration 32900, Loss: 1574.1497802734375\n",
      "Iteration 33000, Loss: 1574.1497802734375\n",
      "Iteration 33100, Loss: 1574.1497802734375\n",
      "Iteration 33200, Loss: 1574.1497802734375\n",
      "Iteration 33300, Loss: 1574.1497802734375\n",
      "Iteration 33400, Loss: 1574.1497802734375\n",
      "Iteration 33500, Loss: 1574.1497802734375\n",
      "Iteration 33600, Loss: 1574.1497802734375\n",
      "Iteration 33700, Loss: 1574.1497802734375\n",
      "Iteration 33800, Loss: 1574.1497802734375\n",
      "Iteration 33900, Loss: 1574.1497802734375\n",
      "Iteration 34000, Loss: 1574.1497802734375\n",
      "Iteration 34100, Loss: 1574.1497802734375\n",
      "Iteration 34200, Loss: 1574.1497802734375\n",
      "Iteration 34300, Loss: 1574.1497802734375\n",
      "Iteration 34400, Loss: 1574.1497802734375\n",
      "Iteration 34500, Loss: 1574.1497802734375\n",
      "Iteration 34600, Loss: 1574.1497802734375\n",
      "Iteration 34700, Loss: 1574.1497802734375\n",
      "Iteration 34800, Loss: 1574.1497802734375\n",
      "Iteration 34900, Loss: 1574.1497802734375\n",
      "Iteration 35000, Loss: 1574.1497802734375\n",
      "Iteration 35100, Loss: 1574.1497802734375\n",
      "Iteration 35200, Loss: 1574.1497802734375\n",
      "Iteration 35300, Loss: 1574.1497802734375\n",
      "Iteration 35400, Loss: 1574.1497802734375\n",
      "Iteration 35500, Loss: 1574.1497802734375\n",
      "Iteration 35600, Loss: 1574.1497802734375\n",
      "Iteration 35700, Loss: 1574.1497802734375\n",
      "Iteration 35800, Loss: 1574.1497802734375\n",
      "Iteration 35900, Loss: 1574.1497802734375\n",
      "Iteration 36000, Loss: 1574.1497802734375\n",
      "Iteration 36100, Loss: 1574.1497802734375\n",
      "Iteration 36200, Loss: 1574.1497802734375\n",
      "Iteration 36300, Loss: 1574.1497802734375\n",
      "Iteration 36400, Loss: 1574.1497802734375\n",
      "Iteration 36500, Loss: 1574.1497802734375\n",
      "Iteration 36600, Loss: 1574.1497802734375\n",
      "Iteration 36700, Loss: 1574.1497802734375\n",
      "Iteration 36800, Loss: 1574.1497802734375\n",
      "Iteration 36900, Loss: 1574.1497802734375\n",
      "Iteration 37000, Loss: 1574.1497802734375\n",
      "Iteration 37100, Loss: 1574.1497802734375\n",
      "Iteration 37200, Loss: 1574.1497802734375\n",
      "Iteration 37300, Loss: 1574.1497802734375\n",
      "Iteration 37400, Loss: 1574.1497802734375\n",
      "Iteration 37500, Loss: 1574.1497802734375\n",
      "Iteration 37600, Loss: 1574.1497802734375\n",
      "Iteration 37700, Loss: 1574.1497802734375\n",
      "Iteration 37800, Loss: 1574.1497802734375\n",
      "Iteration 37900, Loss: 1574.1497802734375\n",
      "Iteration 38000, Loss: 1574.1497802734375\n",
      "Iteration 38100, Loss: 1574.1497802734375\n",
      "Iteration 38200, Loss: 1574.1497802734375\n",
      "Iteration 38300, Loss: 1574.1497802734375\n",
      "Iteration 38400, Loss: 1574.1497802734375\n",
      "Iteration 38500, Loss: 1574.1497802734375\n",
      "Iteration 38600, Loss: 1574.1497802734375\n",
      "Iteration 38700, Loss: 1574.1497802734375\n",
      "Iteration 38800, Loss: 1574.1497802734375\n",
      "Iteration 38900, Loss: 1574.1497802734375\n",
      "Iteration 39000, Loss: 1574.1497802734375\n",
      "Iteration 39100, Loss: 1574.1497802734375\n",
      "Iteration 39200, Loss: 1574.1497802734375\n",
      "Iteration 39300, Loss: 1574.1497802734375\n",
      "Iteration 39400, Loss: 1574.1497802734375\n",
      "Iteration 39500, Loss: 1574.1497802734375\n",
      "Iteration 39600, Loss: 1574.1497802734375\n",
      "Iteration 39700, Loss: 1574.1497802734375\n",
      "Iteration 39800, Loss: 1574.1497802734375\n",
      "Iteration 39900, Loss: 1574.1497802734375\n",
      "Iteration 40000, Loss: 1574.1497802734375\n",
      "Iteration 40100, Loss: 1574.1497802734375\n",
      "Iteration 40200, Loss: 1574.1497802734375\n",
      "Iteration 40300, Loss: 1574.1497802734375\n",
      "Iteration 40400, Loss: 1574.1497802734375\n",
      "Iteration 40500, Loss: 1574.1497802734375\n",
      "Iteration 40600, Loss: 1574.1497802734375\n",
      "Iteration 40700, Loss: 1574.1497802734375\n",
      "Iteration 40800, Loss: 1574.1497802734375\n",
      "Iteration 40900, Loss: 1574.1497802734375\n",
      "Iteration 41000, Loss: 1574.1497802734375\n",
      "Iteration 41100, Loss: 1574.1497802734375\n",
      "Iteration 41200, Loss: 1574.1497802734375\n",
      "Iteration 41300, Loss: 1574.1497802734375\n",
      "Iteration 41400, Loss: 1574.1497802734375\n",
      "Iteration 41500, Loss: 1574.1497802734375\n",
      "Iteration 41600, Loss: 1574.1497802734375\n",
      "Iteration 41700, Loss: 1574.1497802734375\n",
      "Iteration 41800, Loss: 1574.1497802734375\n",
      "Iteration 41900, Loss: 1574.1497802734375\n",
      "Iteration 42000, Loss: 1574.1497802734375\n",
      "Iteration 42100, Loss: 1574.1497802734375\n",
      "Iteration 42200, Loss: 1574.1497802734375\n",
      "Iteration 42300, Loss: 1574.1497802734375\n",
      "Iteration 42400, Loss: 1574.1497802734375\n",
      "Iteration 42500, Loss: 1574.1497802734375\n",
      "Iteration 42600, Loss: 1574.1497802734375\n",
      "Iteration 42700, Loss: 1574.1497802734375\n",
      "Iteration 42800, Loss: 1574.1497802734375\n",
      "Iteration 42900, Loss: 1574.1497802734375\n",
      "Iteration 43000, Loss: 1574.1497802734375\n",
      "Iteration 43100, Loss: 1574.1497802734375\n",
      "Iteration 43200, Loss: 1574.1497802734375\n",
      "Iteration 43300, Loss: 1574.1497802734375\n",
      "Iteration 43400, Loss: 1574.1497802734375\n",
      "Iteration 43500, Loss: 1574.1497802734375\n",
      "Iteration 43600, Loss: 1574.1497802734375\n",
      "Iteration 43700, Loss: 1574.1497802734375\n",
      "Iteration 43800, Loss: 1574.1497802734375\n",
      "Iteration 43900, Loss: 1574.1497802734375\n",
      "Iteration 44000, Loss: 1574.1497802734375\n",
      "Iteration 44100, Loss: 1574.1497802734375\n",
      "Iteration 44200, Loss: 1574.1497802734375\n",
      "Iteration 44300, Loss: 1574.1497802734375\n",
      "Iteration 44400, Loss: 1574.1497802734375\n",
      "Iteration 44500, Loss: 1574.1497802734375\n",
      "Iteration 44600, Loss: 1574.1497802734375\n",
      "Iteration 44700, Loss: 1574.1497802734375\n",
      "Iteration 44800, Loss: 1574.1497802734375\n",
      "Iteration 44900, Loss: 1574.1497802734375\n",
      "Iteration 45000, Loss: 1574.1497802734375\n",
      "Iteration 45100, Loss: 1574.1497802734375\n",
      "Iteration 45200, Loss: 1574.1497802734375\n",
      "Iteration 45300, Loss: 1574.1497802734375\n",
      "Iteration 45400, Loss: 1574.1497802734375\n",
      "Iteration 45500, Loss: 1574.1497802734375\n",
      "Iteration 45600, Loss: 1574.1497802734375\n",
      "Iteration 45700, Loss: 1574.1497802734375\n",
      "Iteration 45800, Loss: 1574.1497802734375\n",
      "Iteration 45900, Loss: 1574.1497802734375\n",
      "Iteration 46000, Loss: 1574.1497802734375\n",
      "Iteration 46100, Loss: 1574.1497802734375\n",
      "Iteration 46200, Loss: 1574.1497802734375\n",
      "Iteration 46300, Loss: 1574.1497802734375\n",
      "Iteration 46400, Loss: 1574.1497802734375\n",
      "Iteration 46500, Loss: 1574.1497802734375\n",
      "Iteration 46600, Loss: 1574.1497802734375\n",
      "Iteration 46700, Loss: 1574.1497802734375\n",
      "Iteration 46800, Loss: 1574.1497802734375\n",
      "Iteration 46900, Loss: 1574.1497802734375\n",
      "Iteration 47000, Loss: 1574.1497802734375\n",
      "Iteration 47100, Loss: 1574.1497802734375\n",
      "Iteration 47200, Loss: 1574.1497802734375\n",
      "Iteration 47300, Loss: 1574.1497802734375\n",
      "Iteration 47400, Loss: 1574.1497802734375\n",
      "Iteration 47500, Loss: 1574.1497802734375\n",
      "Iteration 47600, Loss: 1574.1497802734375\n",
      "Iteration 47700, Loss: 1574.1497802734375\n",
      "Iteration 47800, Loss: 1574.1497802734375\n",
      "Iteration 47900, Loss: 1574.1497802734375\n",
      "Iteration 48000, Loss: 1574.1497802734375\n",
      "Iteration 48100, Loss: 1574.1497802734375\n",
      "Iteration 48200, Loss: 1574.1497802734375\n",
      "Iteration 48300, Loss: 1574.1497802734375\n",
      "Iteration 48400, Loss: 1574.1497802734375\n",
      "Iteration 48500, Loss: 1574.1497802734375\n",
      "Iteration 48600, Loss: 1574.1497802734375\n",
      "Iteration 48700, Loss: 1574.1497802734375\n",
      "Iteration 48800, Loss: 1574.1497802734375\n",
      "Iteration 48900, Loss: 1574.1497802734375\n",
      "Iteration 49000, Loss: 1574.1497802734375\n",
      "Iteration 49100, Loss: 1574.1497802734375\n",
      "Iteration 49200, Loss: 1574.1497802734375\n",
      "Iteration 49300, Loss: 1574.1497802734375\n",
      "Iteration 49400, Loss: 1574.1497802734375\n",
      "Iteration 49500, Loss: 1574.1497802734375\n",
      "Iteration 49600, Loss: 1574.1497802734375\n",
      "Iteration 49700, Loss: 1574.1497802734375\n",
      "Iteration 49800, Loss: 1574.1497802734375\n",
      "Iteration 49900, Loss: 1574.1497802734375\n",
      "Iteration 50000, Loss: 1574.1497802734375\n",
      "Iteration 50100, Loss: 1574.1497802734375\n",
      "Iteration 50200, Loss: 1574.1497802734375\n",
      "Iteration 50300, Loss: 1574.1497802734375\n",
      "Iteration 50400, Loss: 1574.1497802734375\n",
      "Iteration 50500, Loss: 1574.1497802734375\n",
      "Iteration 50600, Loss: 1574.1497802734375\n",
      "Iteration 50700, Loss: 1574.1497802734375\n",
      "Iteration 50800, Loss: 1574.1497802734375\n",
      "Iteration 50900, Loss: 1574.1497802734375\n",
      "Iteration 51000, Loss: 1574.1497802734375\n",
      "Iteration 51100, Loss: 1574.1497802734375\n",
      "Iteration 51200, Loss: 1574.1497802734375\n",
      "Iteration 51300, Loss: 1574.1497802734375\n",
      "Iteration 51400, Loss: 1574.1497802734375\n",
      "Iteration 51500, Loss: 1574.1497802734375\n",
      "Iteration 51600, Loss: 1574.1497802734375\n",
      "Iteration 51700, Loss: 1574.1497802734375\n",
      "Iteration 51800, Loss: 1574.1497802734375\n",
      "Iteration 51900, Loss: 1574.1497802734375\n",
      "Iteration 52000, Loss: 1574.1497802734375\n",
      "Iteration 52100, Loss: 1574.1497802734375\n",
      "Iteration 52200, Loss: 1574.1497802734375\n",
      "Iteration 52300, Loss: 1574.1497802734375\n",
      "Iteration 52400, Loss: 1574.1497802734375\n",
      "Iteration 52500, Loss: 1574.1497802734375\n",
      "Iteration 52600, Loss: 1574.1497802734375\n",
      "Iteration 52700, Loss: 1574.1497802734375\n",
      "Iteration 52800, Loss: 1574.1497802734375\n",
      "Iteration 52900, Loss: 1574.1497802734375\n",
      "Iteration 53000, Loss: 1574.1497802734375\n",
      "Iteration 53100, Loss: 1574.1497802734375\n",
      "Iteration 53200, Loss: 1574.1497802734375\n",
      "Iteration 53300, Loss: 1574.1497802734375\n",
      "Iteration 53400, Loss: 1574.1497802734375\n",
      "Iteration 53500, Loss: 1574.1497802734375\n",
      "Iteration 53600, Loss: 1574.1497802734375\n",
      "Iteration 53700, Loss: 1574.1497802734375\n",
      "Iteration 53800, Loss: 1574.1497802734375\n",
      "Iteration 53900, Loss: 1574.1497802734375\n",
      "Iteration 54000, Loss: 1574.1497802734375\n",
      "Iteration 54100, Loss: 1574.1497802734375\n",
      "Iteration 54200, Loss: 1574.1497802734375\n",
      "Iteration 54300, Loss: 1574.1497802734375\n",
      "Iteration 54400, Loss: 1574.1497802734375\n",
      "Iteration 54500, Loss: 1574.1497802734375\n",
      "Iteration 54600, Loss: 1574.1497802734375\n",
      "Iteration 54700, Loss: 1574.1497802734375\n",
      "Iteration 54800, Loss: 1574.1497802734375\n",
      "Iteration 54900, Loss: 1574.1497802734375\n",
      "Iteration 55000, Loss: 1574.1497802734375\n",
      "Iteration 55100, Loss: 1574.1497802734375\n",
      "Iteration 55200, Loss: 1574.1497802734375\n",
      "Iteration 55300, Loss: 1574.1497802734375\n",
      "Iteration 55400, Loss: 1574.1497802734375\n",
      "Iteration 55500, Loss: 1574.1497802734375\n",
      "Iteration 55600, Loss: 1574.1497802734375\n",
      "Iteration 55700, Loss: 1574.1497802734375\n",
      "Iteration 55800, Loss: 1574.1497802734375\n",
      "Iteration 55900, Loss: 1574.1497802734375\n",
      "Iteration 56000, Loss: 1574.1497802734375\n",
      "Iteration 56100, Loss: 1574.1497802734375\n",
      "Iteration 56200, Loss: 1574.1497802734375\n",
      "Iteration 56300, Loss: 1574.1497802734375\n",
      "Iteration 56400, Loss: 1574.1497802734375\n",
      "Iteration 56500, Loss: 1574.1497802734375\n",
      "Iteration 56600, Loss: 1574.1497802734375\n",
      "Iteration 56700, Loss: 1574.1497802734375\n",
      "Iteration 56800, Loss: 1574.1497802734375\n",
      "Iteration 56900, Loss: 1574.1497802734375\n",
      "Iteration 57000, Loss: 1574.1497802734375\n",
      "Iteration 57100, Loss: 1574.1497802734375\n",
      "Iteration 57200, Loss: 1574.1497802734375\n",
      "Iteration 57300, Loss: 1574.1497802734375\n",
      "Iteration 57400, Loss: 1574.1497802734375\n",
      "Iteration 57500, Loss: 1574.1497802734375\n",
      "Iteration 57600, Loss: 1574.1497802734375\n",
      "Iteration 57700, Loss: 1574.1497802734375\n",
      "Iteration 57800, Loss: 1574.1497802734375\n",
      "Iteration 57900, Loss: 1574.1497802734375\n",
      "Iteration 58000, Loss: 1574.1497802734375\n",
      "Iteration 58100, Loss: 1574.1497802734375\n",
      "Iteration 58200, Loss: 1574.1497802734375\n",
      "Iteration 58300, Loss: 1574.1497802734375\n",
      "Iteration 58400, Loss: 1574.1497802734375\n",
      "Iteration 58500, Loss: 1574.1497802734375\n",
      "Iteration 58600, Loss: 1574.1497802734375\n",
      "Iteration 58700, Loss: 1574.1497802734375\n",
      "Iteration 58800, Loss: 1574.1497802734375\n",
      "Iteration 58900, Loss: 1574.1497802734375\n",
      "Iteration 59000, Loss: 1574.1497802734375\n",
      "Iteration 59100, Loss: 1574.1497802734375\n",
      "Iteration 59200, Loss: 1574.1497802734375\n",
      "Iteration 59300, Loss: 1574.1497802734375\n",
      "Iteration 59400, Loss: 1574.1497802734375\n",
      "Iteration 59500, Loss: 1574.1497802734375\n",
      "Iteration 59600, Loss: 1574.1497802734375\n",
      "Iteration 59700, Loss: 1574.1497802734375\n",
      "Iteration 59800, Loss: 1574.1497802734375\n",
      "Iteration 59900, Loss: 1574.1497802734375\n",
      "Iteration 60000, Loss: 1574.1497802734375\n",
      "Iteration 60100, Loss: 1574.1497802734375\n",
      "Iteration 60200, Loss: 1574.1497802734375\n",
      "Iteration 60300, Loss: 1574.1497802734375\n",
      "Iteration 60400, Loss: 1574.1497802734375\n",
      "Iteration 60500, Loss: 1574.1497802734375\n",
      "Iteration 60600, Loss: 1574.1497802734375\n",
      "Iteration 60700, Loss: 1574.1497802734375\n",
      "Iteration 60800, Loss: 1574.1497802734375\n",
      "Iteration 60900, Loss: 1574.1497802734375\n",
      "Iteration 61000, Loss: 1574.1497802734375\n",
      "Iteration 61100, Loss: 1574.1497802734375\n",
      "Iteration 61200, Loss: 1574.1497802734375\n",
      "Iteration 61300, Loss: 1574.1497802734375\n",
      "Iteration 61400, Loss: 1574.1497802734375\n",
      "Iteration 61500, Loss: 1574.1497802734375\n",
      "Iteration 61600, Loss: 1574.1497802734375\n",
      "Iteration 61700, Loss: 1574.1497802734375\n",
      "Iteration 61800, Loss: 1574.1497802734375\n",
      "Iteration 61900, Loss: 1574.1497802734375\n",
      "Iteration 62000, Loss: 1574.1497802734375\n",
      "Iteration 62100, Loss: 1574.1497802734375\n",
      "Iteration 62200, Loss: 1574.1497802734375\n",
      "Iteration 62300, Loss: 1574.1497802734375\n",
      "Iteration 62400, Loss: 1574.1497802734375\n",
      "Iteration 62500, Loss: 1574.1497802734375\n",
      "Iteration 62600, Loss: 1574.1497802734375\n",
      "Iteration 62700, Loss: 1574.1497802734375\n",
      "Iteration 62800, Loss: 1574.1497802734375\n",
      "Iteration 62900, Loss: 1574.1497802734375\n",
      "Iteration 63000, Loss: 1574.1497802734375\n",
      "Iteration 63100, Loss: 1574.1497802734375\n",
      "Iteration 63200, Loss: 1574.1497802734375\n",
      "Iteration 63300, Loss: 1574.1497802734375\n",
      "Iteration 63400, Loss: 1574.1497802734375\n",
      "Iteration 63500, Loss: 1574.1497802734375\n",
      "Iteration 63600, Loss: 1574.1497802734375\n",
      "Iteration 63700, Loss: 1574.1497802734375\n",
      "Iteration 63800, Loss: 1574.1497802734375\n",
      "Iteration 63900, Loss: 1574.1497802734375\n",
      "Iteration 64000, Loss: 1574.1497802734375\n",
      "Iteration 64100, Loss: 1574.1497802734375\n",
      "Iteration 64200, Loss: 1574.1497802734375\n",
      "Iteration 64300, Loss: 1574.1497802734375\n",
      "Iteration 64400, Loss: 1574.1497802734375\n",
      "Iteration 64500, Loss: 1574.1497802734375\n",
      "Iteration 64600, Loss: 1574.1497802734375\n",
      "Iteration 64700, Loss: 1574.1497802734375\n",
      "Iteration 64800, Loss: 1574.1497802734375\n",
      "Iteration 64900, Loss: 1574.1497802734375\n",
      "Iteration 65000, Loss: 1574.1497802734375\n",
      "Iteration 65100, Loss: 1574.1497802734375\n",
      "Iteration 65200, Loss: 1574.1497802734375\n",
      "Iteration 65300, Loss: 1574.1497802734375\n",
      "Iteration 65400, Loss: 1574.1497802734375\n",
      "Iteration 65500, Loss: 1574.1497802734375\n",
      "Iteration 65600, Loss: 1574.1497802734375\n",
      "Iteration 65700, Loss: 1574.1497802734375\n",
      "Iteration 65800, Loss: 1574.1497802734375\n",
      "Iteration 65900, Loss: 1574.1497802734375\n",
      "Iteration 66000, Loss: 1574.1497802734375\n",
      "Iteration 66100, Loss: 1574.1497802734375\n",
      "Iteration 66200, Loss: 1574.1497802734375\n",
      "Iteration 66300, Loss: 1574.1497802734375\n",
      "Iteration 66400, Loss: 1574.1497802734375\n",
      "Iteration 66500, Loss: 1574.1497802734375\n",
      "Iteration 66600, Loss: 1574.1497802734375\n",
      "Iteration 66700, Loss: 1574.1497802734375\n",
      "Iteration 66800, Loss: 1574.1497802734375\n",
      "Iteration 66900, Loss: 1574.1497802734375\n",
      "Iteration 67000, Loss: 1574.1497802734375\n",
      "Iteration 67100, Loss: 1574.1497802734375\n",
      "Iteration 67200, Loss: 1574.1497802734375\n",
      "Iteration 67300, Loss: 1574.1497802734375\n",
      "Iteration 67400, Loss: 1574.1497802734375\n",
      "Iteration 67500, Loss: 1574.1497802734375\n",
      "Iteration 67600, Loss: 1574.1497802734375\n",
      "Iteration 67700, Loss: 1574.1497802734375\n",
      "Iteration 67800, Loss: 1574.1497802734375\n",
      "Iteration 67900, Loss: 1574.1497802734375\n",
      "Iteration 68000, Loss: 1574.1497802734375\n",
      "Iteration 68100, Loss: 1574.1497802734375\n",
      "Iteration 68200, Loss: 1574.1497802734375\n",
      "Iteration 68300, Loss: 1574.1497802734375\n",
      "Iteration 68400, Loss: 1574.1497802734375\n",
      "Iteration 68500, Loss: 1574.1497802734375\n",
      "Iteration 68600, Loss: 1574.1497802734375\n",
      "Iteration 68700, Loss: 1574.1497802734375\n",
      "Iteration 68800, Loss: 1574.1497802734375\n",
      "Iteration 68900, Loss: 1574.1497802734375\n",
      "Iteration 69000, Loss: 1574.1497802734375\n",
      "Iteration 69100, Loss: 1574.1497802734375\n",
      "Iteration 69200, Loss: 1574.1497802734375\n",
      "Iteration 69300, Loss: 1574.1497802734375\n",
      "Iteration 69400, Loss: 1574.1497802734375\n",
      "Iteration 69500, Loss: 1574.1497802734375\n",
      "Iteration 69600, Loss: 1574.1497802734375\n",
      "Iteration 69700, Loss: 1574.1497802734375\n",
      "Iteration 69800, Loss: 1574.1497802734375\n",
      "Iteration 69900, Loss: 1574.1497802734375\n",
      "Iteration 70000, Loss: 1574.1497802734375\n",
      "Iteration 70100, Loss: 1574.1497802734375\n",
      "Iteration 70200, Loss: 1574.1497802734375\n",
      "Iteration 70300, Loss: 1574.1497802734375\n",
      "Iteration 70400, Loss: 1574.1497802734375\n",
      "Iteration 70500, Loss: 1574.1497802734375\n",
      "Iteration 70600, Loss: 1574.1497802734375\n",
      "Iteration 70700, Loss: 1574.1497802734375\n",
      "Iteration 70800, Loss: 1574.1497802734375\n",
      "Iteration 70900, Loss: 1574.1497802734375\n",
      "Iteration 71000, Loss: 1574.1497802734375\n",
      "Iteration 71100, Loss: 1574.1497802734375\n",
      "Iteration 71200, Loss: 1574.1497802734375\n",
      "Iteration 71300, Loss: 1574.1497802734375\n",
      "Iteration 71400, Loss: 1574.1497802734375\n",
      "Iteration 71500, Loss: 1574.1497802734375\n",
      "Iteration 71600, Loss: 1574.1497802734375\n",
      "Iteration 71700, Loss: 1574.1497802734375\n",
      "Iteration 71800, Loss: 1574.1497802734375\n",
      "Iteration 71900, Loss: 1574.1497802734375\n",
      "Iteration 72000, Loss: 1574.1497802734375\n",
      "Iteration 72100, Loss: 1574.1497802734375\n",
      "Iteration 72200, Loss: 1574.1497802734375\n",
      "Iteration 72300, Loss: 1574.1497802734375\n",
      "Iteration 72400, Loss: 1574.1497802734375\n",
      "Iteration 72500, Loss: 1574.1497802734375\n",
      "Iteration 72600, Loss: 1574.1497802734375\n",
      "Iteration 72700, Loss: 1574.1497802734375\n",
      "Iteration 72800, Loss: 1574.1497802734375\n",
      "Iteration 72900, Loss: 1574.1497802734375\n",
      "Iteration 73000, Loss: 1574.1497802734375\n",
      "Iteration 73100, Loss: 1574.1497802734375\n",
      "Iteration 73200, Loss: 1574.1497802734375\n",
      "Iteration 73300, Loss: 1574.1497802734375\n",
      "Iteration 73400, Loss: 1574.1497802734375\n",
      "Iteration 73500, Loss: 1574.1497802734375\n",
      "Iteration 73600, Loss: 1574.1497802734375\n",
      "Iteration 73700, Loss: 1574.1497802734375\n",
      "Iteration 73800, Loss: 1574.1497802734375\n",
      "Iteration 73900, Loss: 1574.1497802734375\n",
      "Iteration 74000, Loss: 1574.1497802734375\n",
      "Iteration 74100, Loss: 1574.1497802734375\n",
      "Iteration 74200, Loss: 1574.1497802734375\n",
      "Iteration 74300, Loss: 1574.1497802734375\n",
      "Iteration 74400, Loss: 1574.1497802734375\n",
      "Iteration 74500, Loss: 1574.1497802734375\n",
      "Iteration 74600, Loss: 1574.1497802734375\n",
      "Iteration 74700, Loss: 1574.1497802734375\n",
      "Iteration 74800, Loss: 1574.1497802734375\n",
      "Iteration 74900, Loss: 1574.1497802734375\n",
      "Iteration 75000, Loss: 1574.1497802734375\n",
      "Iteration 75100, Loss: 1574.1497802734375\n",
      "Iteration 75200, Loss: 1574.1497802734375\n",
      "Iteration 75300, Loss: 1574.1497802734375\n",
      "Iteration 75400, Loss: 1574.1497802734375\n",
      "Iteration 75500, Loss: 1574.1497802734375\n",
      "Iteration 75600, Loss: 1574.1497802734375\n",
      "Iteration 75700, Loss: 1574.1497802734375\n",
      "Iteration 75800, Loss: 1574.1497802734375\n",
      "Iteration 75900, Loss: 1574.1497802734375\n",
      "Iteration 76000, Loss: 1574.1497802734375\n",
      "Iteration 76100, Loss: 1574.1497802734375\n",
      "Iteration 76200, Loss: 1574.1497802734375\n",
      "Iteration 76300, Loss: 1574.1497802734375\n",
      "Iteration 76400, Loss: 1574.1497802734375\n",
      "Iteration 76500, Loss: 1574.1497802734375\n",
      "Iteration 76600, Loss: 1574.1497802734375\n",
      "Iteration 76700, Loss: 1574.1497802734375\n",
      "Iteration 76800, Loss: 1574.1497802734375\n",
      "Iteration 76900, Loss: 1574.1497802734375\n",
      "Iteration 77000, Loss: 1574.1497802734375\n",
      "Iteration 77100, Loss: 1574.1497802734375\n",
      "Iteration 77200, Loss: 1574.1497802734375\n",
      "Iteration 77300, Loss: 1574.1497802734375\n",
      "Iteration 77400, Loss: 1574.1497802734375\n",
      "Iteration 77500, Loss: 1574.1497802734375\n",
      "Iteration 77600, Loss: 1574.1497802734375\n",
      "Iteration 77700, Loss: 1574.1497802734375\n",
      "Iteration 77800, Loss: 1574.1497802734375\n",
      "Iteration 77900, Loss: 1574.1497802734375\n",
      "Iteration 78000, Loss: 1574.1497802734375\n",
      "Iteration 78100, Loss: 1574.1497802734375\n",
      "Iteration 78200, Loss: 1574.1497802734375\n",
      "Iteration 78300, Loss: 1574.1497802734375\n",
      "Iteration 78400, Loss: 1574.1497802734375\n",
      "Iteration 78500, Loss: 1574.1497802734375\n",
      "Iteration 78600, Loss: 1574.1497802734375\n",
      "Iteration 78700, Loss: 1574.1497802734375\n",
      "Iteration 78800, Loss: 1574.1497802734375\n",
      "Iteration 78900, Loss: 1574.1497802734375\n",
      "Iteration 79000, Loss: 1574.1497802734375\n",
      "Iteration 79100, Loss: 1574.1497802734375\n",
      "Iteration 79200, Loss: 1574.1497802734375\n",
      "Iteration 79300, Loss: 1574.1497802734375\n",
      "Iteration 79400, Loss: 1574.1497802734375\n",
      "Iteration 79500, Loss: 1574.1497802734375\n",
      "Iteration 79600, Loss: 1574.1497802734375\n",
      "Iteration 79700, Loss: 1574.1497802734375\n",
      "Iteration 79800, Loss: 1574.1497802734375\n",
      "Iteration 79900, Loss: 1574.1497802734375\n",
      "Iteration 80000, Loss: 1574.1497802734375\n",
      "Iteration 80100, Loss: 1574.1497802734375\n",
      "Iteration 80200, Loss: 1574.1497802734375\n",
      "Iteration 80300, Loss: 1574.1497802734375\n",
      "Iteration 80400, Loss: 1574.1497802734375\n",
      "Iteration 80500, Loss: 1574.1497802734375\n",
      "Iteration 80600, Loss: 1574.1497802734375\n",
      "Iteration 80700, Loss: 1574.1497802734375\n",
      "Iteration 80800, Loss: 1574.1497802734375\n",
      "Iteration 80900, Loss: 1574.1497802734375\n",
      "Iteration 81000, Loss: 1574.1497802734375\n",
      "Iteration 81100, Loss: 1574.1497802734375\n",
      "Iteration 81200, Loss: 1574.1497802734375\n",
      "Iteration 81300, Loss: 1574.1497802734375\n",
      "Iteration 81400, Loss: 1574.1497802734375\n",
      "Iteration 81500, Loss: 1574.1497802734375\n",
      "Iteration 81600, Loss: 1574.1497802734375\n",
      "Iteration 81700, Loss: 1574.1497802734375\n",
      "Iteration 81800, Loss: 1574.1497802734375\n",
      "Iteration 81900, Loss: 1574.1497802734375\n",
      "Iteration 82000, Loss: 1574.1497802734375\n",
      "Iteration 82100, Loss: 1574.1497802734375\n",
      "Iteration 82200, Loss: 1574.1497802734375\n",
      "Iteration 82300, Loss: 1574.1497802734375\n",
      "Iteration 82400, Loss: 1574.1497802734375\n",
      "Iteration 82500, Loss: 1574.1497802734375\n",
      "Iteration 82600, Loss: 1574.1497802734375\n",
      "Iteration 82700, Loss: 1574.1497802734375\n",
      "Iteration 82800, Loss: 1574.1497802734375\n",
      "Iteration 82900, Loss: 1574.1497802734375\n",
      "Iteration 83000, Loss: 1574.1497802734375\n",
      "Iteration 83100, Loss: 1574.1497802734375\n",
      "Iteration 83200, Loss: 1574.1497802734375\n",
      "Iteration 83300, Loss: 1574.1497802734375\n",
      "Iteration 83400, Loss: 1574.1497802734375\n",
      "Iteration 83500, Loss: 1574.1497802734375\n",
      "Iteration 83600, Loss: 1574.1497802734375\n",
      "Iteration 83700, Loss: 1574.1497802734375\n",
      "Iteration 83800, Loss: 1574.1497802734375\n",
      "Iteration 83900, Loss: 1574.1497802734375\n",
      "Iteration 84000, Loss: 1574.1497802734375\n",
      "Iteration 84100, Loss: 1574.1497802734375\n",
      "Iteration 84200, Loss: 1574.1497802734375\n",
      "Iteration 84300, Loss: 1574.1497802734375\n",
      "Iteration 84400, Loss: 1574.1497802734375\n",
      "Iteration 84500, Loss: 1574.1497802734375\n",
      "Iteration 84600, Loss: 1574.1497802734375\n",
      "Iteration 84700, Loss: 1574.1497802734375\n",
      "Iteration 84800, Loss: 1574.1497802734375\n",
      "Iteration 84900, Loss: 1574.1497802734375\n",
      "Iteration 85000, Loss: 1574.1497802734375\n",
      "Iteration 85100, Loss: 1574.1497802734375\n",
      "Iteration 85200, Loss: 1574.1497802734375\n",
      "Iteration 85300, Loss: 1574.1497802734375\n",
      "Iteration 85400, Loss: 1574.1497802734375\n",
      "Iteration 85500, Loss: 1574.1497802734375\n",
      "Iteration 85600, Loss: 1574.1497802734375\n",
      "Iteration 85700, Loss: 1574.1497802734375\n",
      "Iteration 85800, Loss: 1574.1497802734375\n",
      "Iteration 85900, Loss: 1574.1497802734375\n",
      "Iteration 86000, Loss: 1574.1497802734375\n",
      "Iteration 86100, Loss: 1574.1497802734375\n",
      "Iteration 86200, Loss: 1574.1497802734375\n",
      "Iteration 86300, Loss: 1574.1497802734375\n",
      "Iteration 86400, Loss: 1574.1497802734375\n",
      "Iteration 86500, Loss: 1574.1497802734375\n",
      "Iteration 86600, Loss: 1574.1497802734375\n",
      "Iteration 86700, Loss: 1574.1497802734375\n",
      "Iteration 86800, Loss: 1574.1497802734375\n",
      "Iteration 86900, Loss: 1574.1497802734375\n",
      "Iteration 87000, Loss: 1574.1497802734375\n",
      "Iteration 87100, Loss: 1574.1497802734375\n",
      "Iteration 87200, Loss: 1574.1497802734375\n",
      "Iteration 87300, Loss: 1574.1497802734375\n",
      "Iteration 87400, Loss: 1574.1497802734375\n",
      "Iteration 87500, Loss: 1574.1497802734375\n",
      "Iteration 87600, Loss: 1574.1497802734375\n",
      "Iteration 87700, Loss: 1574.1497802734375\n",
      "Iteration 87800, Loss: 1574.1497802734375\n",
      "Iteration 87900, Loss: 1574.1497802734375\n",
      "Iteration 88000, Loss: 1574.1497802734375\n",
      "Iteration 88100, Loss: 1574.1497802734375\n",
      "Iteration 88200, Loss: 1574.1497802734375\n",
      "Iteration 88300, Loss: 1574.1497802734375\n",
      "Iteration 88400, Loss: 1574.1497802734375\n",
      "Iteration 88500, Loss: 1574.1497802734375\n",
      "Iteration 88600, Loss: 1574.1497802734375\n",
      "Iteration 88700, Loss: 1574.1497802734375\n",
      "Iteration 88800, Loss: 1574.1497802734375\n",
      "Iteration 88900, Loss: 1574.1497802734375\n",
      "Iteration 89000, Loss: 1574.1497802734375\n",
      "Iteration 89100, Loss: 1574.1497802734375\n",
      "Iteration 89200, Loss: 1574.1497802734375\n",
      "Iteration 89300, Loss: 1574.1497802734375\n",
      "Iteration 89400, Loss: 1574.1497802734375\n",
      "Iteration 89500, Loss: 1574.1497802734375\n",
      "Iteration 89600, Loss: 1574.1497802734375\n",
      "Iteration 89700, Loss: 1574.1497802734375\n",
      "Iteration 89800, Loss: 1574.1497802734375\n",
      "Iteration 89900, Loss: 1574.1497802734375\n",
      "Iteration 90000, Loss: 1574.1497802734375\n",
      "Iteration 90100, Loss: 1574.1497802734375\n",
      "Iteration 90200, Loss: 1574.1497802734375\n",
      "Iteration 90300, Loss: 1574.1497802734375\n",
      "Iteration 90400, Loss: 1574.1497802734375\n",
      "Iteration 90500, Loss: 1574.1497802734375\n",
      "Iteration 90600, Loss: 1574.1497802734375\n",
      "Iteration 90700, Loss: 1574.1497802734375\n",
      "Iteration 90800, Loss: 1574.1497802734375\n",
      "Iteration 90900, Loss: 1574.1497802734375\n",
      "Iteration 91000, Loss: 1574.1497802734375\n",
      "Iteration 91100, Loss: 1574.1497802734375\n",
      "Iteration 91200, Loss: 1574.1497802734375\n",
      "Iteration 91300, Loss: 1574.1497802734375\n",
      "Iteration 91400, Loss: 1574.1497802734375\n",
      "Iteration 91500, Loss: 1574.1497802734375\n",
      "Iteration 91600, Loss: 1574.1497802734375\n",
      "Iteration 91700, Loss: 1574.1497802734375\n",
      "Iteration 91800, Loss: 1574.1497802734375\n",
      "Iteration 91900, Loss: 1574.1497802734375\n",
      "Iteration 92000, Loss: 1574.1497802734375\n",
      "Iteration 92100, Loss: 1574.1497802734375\n",
      "Iteration 92200, Loss: 1574.1497802734375\n",
      "Iteration 92300, Loss: 1574.1497802734375\n",
      "Iteration 92400, Loss: 1574.1497802734375\n",
      "Iteration 92500, Loss: 1574.1497802734375\n",
      "Iteration 92600, Loss: 1574.1497802734375\n",
      "Iteration 92700, Loss: 1574.1497802734375\n",
      "Iteration 92800, Loss: 1574.1497802734375\n",
      "Iteration 92900, Loss: 1574.1497802734375\n",
      "Iteration 93000, Loss: 1574.1497802734375\n",
      "Iteration 93100, Loss: 1574.1497802734375\n",
      "Iteration 93200, Loss: 1574.1497802734375\n",
      "Iteration 93300, Loss: 1574.1497802734375\n",
      "Iteration 93400, Loss: 1574.1497802734375\n",
      "Iteration 93500, Loss: 1574.1497802734375\n",
      "Iteration 93600, Loss: 1574.1497802734375\n",
      "Iteration 93700, Loss: 1574.1497802734375\n",
      "Iteration 93800, Loss: 1574.1497802734375\n",
      "Iteration 93900, Loss: 1574.1497802734375\n",
      "Iteration 94000, Loss: 1574.1497802734375\n",
      "Iteration 94100, Loss: 1574.1497802734375\n",
      "Iteration 94200, Loss: 1574.1497802734375\n",
      "Iteration 94300, Loss: 1574.1497802734375\n",
      "Iteration 94400, Loss: 1574.1497802734375\n",
      "Iteration 94500, Loss: 1574.1497802734375\n",
      "Iteration 94600, Loss: 1574.1497802734375\n",
      "Iteration 94700, Loss: 1574.1497802734375\n",
      "Iteration 94800, Loss: 1574.1497802734375\n",
      "Iteration 94900, Loss: 1574.1497802734375\n",
      "Iteration 95000, Loss: 1574.1497802734375\n",
      "Iteration 95100, Loss: 1574.1497802734375\n",
      "Iteration 95200, Loss: 1574.1497802734375\n",
      "Iteration 95300, Loss: 1574.1497802734375\n",
      "Iteration 95400, Loss: 1574.1497802734375\n",
      "Iteration 95500, Loss: 1574.1497802734375\n",
      "Iteration 95600, Loss: 1574.1497802734375\n",
      "Iteration 95700, Loss: 1574.1497802734375\n",
      "Iteration 95800, Loss: 1574.1497802734375\n",
      "Iteration 95900, Loss: 1574.1497802734375\n",
      "Iteration 96000, Loss: 1574.1497802734375\n",
      "Iteration 96100, Loss: 1574.1497802734375\n",
      "Iteration 96200, Loss: 1574.1497802734375\n",
      "Iteration 96300, Loss: 1574.1497802734375\n",
      "Iteration 96400, Loss: 1574.1497802734375\n",
      "Iteration 96500, Loss: 1574.1497802734375\n",
      "Iteration 96600, Loss: 1574.1497802734375\n",
      "Iteration 96700, Loss: 1574.1497802734375\n",
      "Iteration 96800, Loss: 1574.1497802734375\n",
      "Iteration 96900, Loss: 1574.1497802734375\n",
      "Iteration 97000, Loss: 1574.1497802734375\n",
      "Iteration 97100, Loss: 1574.1497802734375\n",
      "Iteration 97200, Loss: 1574.1497802734375\n",
      "Iteration 97300, Loss: 1574.1497802734375\n",
      "Iteration 97400, Loss: 1574.1497802734375\n",
      "Iteration 97500, Loss: 1574.1497802734375\n",
      "Iteration 97600, Loss: 1574.1497802734375\n",
      "Iteration 97700, Loss: 1574.1497802734375\n",
      "Iteration 97800, Loss: 1574.1497802734375\n",
      "Iteration 97900, Loss: 1574.1497802734375\n",
      "Iteration 98000, Loss: 1574.1497802734375\n",
      "Iteration 98100, Loss: 1574.1497802734375\n",
      "Iteration 98200, Loss: 1574.1497802734375\n",
      "Iteration 98300, Loss: 1574.1497802734375\n",
      "Iteration 98400, Loss: 1574.1497802734375\n",
      "Iteration 98500, Loss: 1574.1497802734375\n",
      "Iteration 98600, Loss: 1574.1497802734375\n",
      "Iteration 98700, Loss: 1574.1497802734375\n",
      "Iteration 98800, Loss: 1574.1497802734375\n",
      "Iteration 98900, Loss: 1574.1497802734375\n",
      "Iteration 99000, Loss: 1574.1497802734375\n",
      "Iteration 99100, Loss: 1574.1497802734375\n",
      "Iteration 99200, Loss: 1574.1497802734375\n",
      "Iteration 99300, Loss: 1574.1497802734375\n",
      "Iteration 99400, Loss: 1574.1497802734375\n",
      "Iteration 99500, Loss: 1574.1497802734375\n",
      "Iteration 99600, Loss: 1574.1497802734375\n",
      "Iteration 99700, Loss: 1574.1497802734375\n",
      "Iteration 99800, Loss: 1574.1497802734375\n",
      "Iteration 99900, Loss: 1574.1497802734375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the function y = (2x - 5)**2\n",
    "def function(x):\n",
    "    return (2*x - 5)**2\n",
    "\n",
    "# Define the neural network that acts as the optimizer\n",
    "class Optimizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Optimizer, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the optimizer\n",
    "optimizer = Optimizer()\n",
    "\n",
    "# Define the optimizer's loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the parameters to optimize\n",
    "x = torch.randn(1, 1, requires_grad=True)\n",
    "\n",
    "# Training loop\n",
    "for i in range(100000):\n",
    "    # Compute the function value\n",
    "    y = function(x)\n",
    "    # Forward pass\n",
    "    optimizer_output = optimizer(x)\n",
    "    # Compute the update\n",
    "    update = optimizer_output * 0.01\n",
    "    # Update the parameters\n",
    "    x.data = x.data - update\n",
    "    # Compute the loss\n",
    "    loss = criterion(x, y)\n",
    "    # Update the optimizer's parameters\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'Iteration {i}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 15:48:18.847200: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-27 15:48:18.951084: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-27 15:48:18.954483: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-27 15:48:18.954496: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-27 15:48:19.569188: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-27 15:48:19.569259: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-27 15:48:19.569267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-27 15:48:19.939092: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-27 15:48:19.960510: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-27 15:48:19.961292: W external/org_tensorflow/tensorflow/tsl/platform/default/dso_loader.cc:66] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 15:49:06.879782: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-27 15:49:06.879882: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-27 15:49:06.879933: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-27 15:49:06.879980: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-01-27 15:49:06.880023: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-01-27 15:49:06.880067: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-27 15:49:06.880109: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-27 15:49:06.880159: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-01-27 15:49:06.880171: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-01-27 15:49:06.880796: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'optimizer_network' (type OptimizerNetwork).\n\nInput 0 of layer \"dense\" is incompatible with the layer: expected min_ndim=2, found ndim=0. Full shape received: ()\n\nCall arguments received by layer 'optimizer_network' (type OptimizerNetwork):\n   inputs=<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=10.0>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m     31\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m---> 32\u001b[0m         x_new \u001b[39m=\u001b[39m optimizer(x)\n\u001b[1;32m     33\u001b[0m         loss_value \u001b[39m=\u001b[39m loss(x, x_new)\n\u001b[1;32m     34\u001b[0m     grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss_value, optimizer\u001b[39m.\u001b[39mtrainable_variables)\n",
      "File \u001b[0;32m~/anaconda3/envs/ap/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m, in \u001b[0;36mOptimizerNetwork.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m---> 14\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(inputs)\n\u001b[1;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'optimizer_network' (type OptimizerNetwork).\n\nInput 0 of layer \"dense\" is incompatible with the layer: expected min_ndim=2, found ndim=0. Full shape received: ()\n\nCall arguments received by layer 'optimizer_network' (type OptimizerNetwork):\n   inputs=<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=10.0>"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the function to be minimized\n",
    "def f(x):\n",
    "    return (2*x - 5)**2\n",
    "\n",
    "# Define the optimizer network\n",
    "class OptimizerNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(OptimizerNetwork, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        return x\n",
    "\n",
    "optimizer = OptimizerNetwork()\n",
    "\n",
    "# Define the loss function\n",
    "def loss(x, x_new):\n",
    "    return f(x_new)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer_ = tf.optimizers.Adam()\n",
    "\n",
    "# Set the initial value for x\n",
    "x = tf.Variable(10.0)\n",
    "\n",
    "# Train the optimizer\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_new = optimizer(x)\n",
    "        loss_value = loss(x, x_new)\n",
    "    grads = tape.gradient(loss_value, optimizer.trainable_variables)\n",
    "    optimizer_.apply_gradients(zip(grads, optimizer.trainable_variables))\n",
    "    x.assign(x_new)\n",
    "\n",
    "# Print the minimum value of x\n",
    "print(\"Minimum value of x:\", x.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the function to be minimized\n",
    "def f(x):\n",
    "    return (2*x - 5)**2\n",
    "\n",
    "# Define the optimizer network\n",
    "class OptimizerNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(OptimizerNetwork, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        return x\n",
    "\n",
    "optimizer = OptimizerNetwork()\n",
    "\n",
    "# Define the loss function\n",
    "def loss(x, x_new):\n",
    "    return f(x_new)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer_ = tf.optimizers.Adam()\n",
    "\n",
    "# Set the initial value for x\n",
    "x = tf.Variable(10.0)\n",
    "\n",
    "# Train the optimizer\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_new = optimizer(x)\n",
    "        loss_value = loss(x, x_new)\n",
    "    grads = tape.gradient(loss_value, optimizer.trainable_variables)\n",
    "    optimizer_.apply_gradients(zip(grads, optimizer.trainable_variables))\n",
    "    x.assign(x_new)\n",
    "\n",
    "# Print the minimum value of x\n",
    "print(\"Minimum value of x:\", x.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fa2ed8e8ee7173a05202e5daba96d7f457997bf848b47110e51c7b6f0a2fe84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
